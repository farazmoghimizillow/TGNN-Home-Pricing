{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0468d270-f5f7-47a9-bff5-8863cd82dc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nknn baseline for comps\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version='knn_comps'\n",
    "\n",
    "'''\n",
    "knn baseline for comps\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae23592-808d-4319-93da-00e4f2d859a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import collections\n",
    "import copy\n",
    "import csv\n",
    "import math\n",
    "import numbers\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "import types\n",
    "import warnings\n",
    "from functools import partial, update_wrapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader, NeighborSampler\n",
    "from torch_geometric.nn import SAGEConv, TransformerConv,NNConv\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from tqdm import tqdm\n",
    "import torch_geometric\n",
    "torch_geometric.__version__\n",
    "torch.__version__\n",
    "print(torch.version.cuda)\n",
    "import sys\n",
    "# sys.path\n",
    "# sys.path.append('zestimate-neural-net')\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.sql import DataFrame, SparkSession\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql import types as T\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# from aip_spark_sdk import AwsHelper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import csv\n",
    "import math\n",
    "# import numbers\n",
    "# import random\n",
    "# import threading\n",
    "# import time\n",
    "# import types\n",
    "import warnings\n",
    "from functools import partial, update_wrapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "# from sklearn.metrics import mean_absolute_percentage_error\n",
    "# from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import BallTree\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f477cbc-d9ad-4ac3-adee-d396272dcd97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d56caba-1914-41f1-bc1c-cb1036d3694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict():\n",
    "    num_features = [\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"year_built\",\n",
    "        \"year_reno\",\n",
    "        \"grade\",\n",
    "        \"fbsmt_grade\",\n",
    "        \"condition\",\n",
    "        \"stories\",\n",
    "        \"beds\",\n",
    "        \"bath_full\",\n",
    "        \"bath_3qtr\",\n",
    "        \"bath_half\",\n",
    "    ]\n",
    "    num_log_features = [\n",
    "        \"land_val\",\n",
    "        \"imp_val\",\n",
    "        \"sqft_lot\",\n",
    "        \"sqft\",\n",
    "        \"sqft_1\",\n",
    "        \"sqft_fbsmt\",\n",
    "        \"garb_sqft\",\n",
    "        \"gara_sqft\",\n",
    "    ]\n",
    "    cat_features = [\n",
    "        # \"area\",\n",
    "        \"city\",\n",
    "       \"submarket\",\n",
    "        \"zoning\",\n",
    "        # \"present_use\",\n",
    "\n",
    "        # \"wfnt\",\n",
    "        # \"golf\",\n",
    "        # \"greenbelt\",\n",
    "        # \"noise_traffic\",\n",
    "        # \"view_rainier\",\n",
    "        # \"view_olympics\",\n",
    "        # \"view_cascades\",\n",
    "        # \"view_territorial\",\n",
    "        # \"view_skyline\",\n",
    "        # \"view_sound\",\n",
    "        # \"view_lakewash\",\n",
    "        # \"view_lakesamm\",\n",
    "        # \"view_otherwater\",\n",
    "        # \"view_other\",\n",
    "        \n",
    "        # \"sale_date_yyyymm\",\n",
    "        # \"sale_decade\",\n",
    "        # \"sale_year\",\n",
    "        # \"sale_week\",\n",
    "    ]\n",
    "    ord_features = [\n",
    "        # \"stories\",\n",
    "        # \"beds\",\n",
    "        # \"bath_full\",\n",
    "        # \"bath_3qtr\",\n",
    "        # \"bath_half\",\n",
    "    ]\n",
    "    time_features = [\"sale_date\",]\n",
    "\n",
    "    feature_dict = {\n",
    "        \"nums\": num_features,\n",
    "        \"num_logs\": num_log_features,\n",
    "        \"cats\": cat_features,\n",
    "        \"ords\": ord_features,\n",
    "        \"time\": time_features,\n",
    "    }\n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "\n",
    "feature_dict = get_feature_dict()\n",
    "\n",
    "id_cols = [\"sale_id\", \"pinx\", \"submarket\"]\n",
    "response_col = \"sale_price\"\n",
    "feature_cols = set(\n",
    "    feature_dict[\"nums\"] +\n",
    "    feature_dict[\"num_logs\"] +\n",
    "    feature_dict[\"cats\"] +\n",
    "    feature_dict[\"ords\"] +\n",
    "    feature_dict[\"time\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e07151-7655-4b2d-a647-3f4140fdd338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseTransformer:\n",
    "    \"\"\"Response transformer.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.median = None\n",
    "\n",
    "    def fit(self, y):\n",
    "        y_trans = np.log1p(y)\n",
    "        self.median = np.median(y_trans)\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        y = np.log1p(y) - self.median\n",
    "        return y\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        return self.fit(y).transform(y)\n",
    "\n",
    "    def inverse_transform(self, y):\n",
    "        y = np.expm1(y + self.median)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031a7781-482d-4e45-9092-8a4359aadb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('kingcountysales.csv')\n",
    "df[\"sale_date\"]=pd.to_datetime(df[\"sale_date\"])\n",
    "max_train_date = \"2021-01-01\"\n",
    "min_train_date = \"2016-01-01\"\n",
    "sample_beg_date= \"2015-01-01\"\n",
    "df_1= df[df[\"sale_date\"] >= min_train_date].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422f2047-152f-4ad3-a178-815c2d02656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_missing(df):\n",
    "    df = (\n",
    "        df\n",
    "        .assign(imp_val=lambda x: x['imp_val'].replace(0, np.nan))\n",
    "        .assign(land_val=lambda x: x['land_val'].replace(0, np.nan))\n",
    "        .assign(sqft=lambda x: x['sqft'].replace(0, np.nan))\n",
    "        .assign(beds=lambda x: x['beds'].replace(54, np.nan))\n",
    "    )\n",
    "    return df\n",
    "def clean_sqft(df):\n",
    "    sqft_1 = df.query('sqft < sqft_1')['sqft_1']\n",
    "    sqft_fbsmt = df.query('sqft < sqft_1')['sqft_fbsmt']\n",
    "    df.loc[df['sqft'] < df['sqft_1'], 'sqft'] = sqft_1 + sqft_fbsmt\n",
    "    return df\n",
    "\n",
    "def clean_year_built(df):\n",
    "    df['build_type'] = 'Standard'\n",
    "    df.loc[(df['year_built'] - 1) == pd.to_datetime(df['sale_date']).dt.year, 'build_type'] = 'New Construction'\n",
    "    df.loc[(df['year_built'] - 1) > pd.to_datetime(df['sale_date']).dt.year, 'build_type'] = 'Lot Sale'\n",
    "    return df\n",
    "\n",
    "def prepare_outliers(df):\n",
    "    df = (\n",
    "        df\n",
    "        .pipe(clean_sqft)\n",
    "        .pipe(clean_year_built)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def preprocessor(df_1, feature_c):\n",
    "    df_out=df_1.copy()\n",
    "    # feature_cols=feature_c.copy()\n",
    "    \n",
    "    '''\n",
    "    categorical features: replace each category with its price statistics from prior years  \n",
    "    '''\n",
    "    df_sample=  df[(df[\"sale_date\"] < min_train_date) & (df[\"sale_date\"] >= sample_beg_date)].copy()\n",
    "    trans_1=ResponseTransformer()\n",
    "    df_sample[response_col]=trans_1.fit_transform(df_sample[response_col].values)\n",
    "    # df_sample[response_col]= np.log1p(df_sample[response_col].values) - df_sample[response_col].median()\n",
    "    for cat_col in feature_dict['cats']:\n",
    "        cat_stats= df_sample[[cat_col,response_col]].groupby(cat_col).describe().add_prefix(f'{cat_col}_')[f'{cat_col}_sale_price'].reset_index()\n",
    "        df_out=df_out.merge(cat_stats, how='left', on=cat_col)\n",
    "        print(cat_col)\n",
    "        print(df_out.shape)\n",
    "        print(df_out['sale_date'].isna().sum())\n",
    "        df_out.drop(columns=cat_col, inplace=True)\n",
    "        feature_cols.update(list(cat_stats.columns))\n",
    "        feature_cols.remove(cat_col)\n",
    "        \n",
    "    '''\n",
    "    temporal adjusting \n",
    "    '''\n",
    "    print(df_out['sale_date'].isna().sum())\n",
    "    date_time = df_out['sale_date']\n",
    "\n",
    "    timestamp_s = date_time.dt.week\n",
    "    # df_1['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "    # df_1['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "    df_out['year_sin'] = np.sin(timestamp_s * (2 * np.pi / 52))\n",
    "    df_out['year_cos'] = np.cos(timestamp_s * (2 * np.pi / 52))\n",
    "    # df_out.drop(columns='sale_date', inplace=True)\n",
    "    df_out['trans_year']=date_time.dt.year\n",
    "    feature_cols.update(['trans_year','year_sin','year_cos'])\n",
    "    feature_cols.remove('sale_date')\n",
    "    \n",
    "    '''\n",
    "    taking log\n",
    "    '''\n",
    "    \n",
    "    df_out[feature_dict['num_logs']]=np.log1p(df_out[feature_dict['num_logs']])\n",
    "    '''\n",
    "    filling missing values\n",
    "    '''\n",
    "    \n",
    "    df_out.fillna(df_out.median(), inplace=True)\n",
    "    # print(df_out.isna().sum())\n",
    "    # df_out.dropna(inplace=True)\n",
    "    return df_out, feature_cols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61d2a98-8b30-4e5d-a78f-b38c685176e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161512, 47)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a931304-3d76-45dd-ab3f-40e244633ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city\n",
      "(161512, 56)\n",
      "0\n",
      "submarket\n",
      "(161512, 63)\n",
      "0\n",
      "zoning\n",
      "(161512, 70)\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132/1722977719.py:57: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  timestamp_s = date_time.dt.week\n",
      "/tmp/ipykernel_132/1722977719.py:76: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  df_out.fillna(df_out.median(), inplace=True)\n",
      "/tmp/ipykernel_132/1722977719.py:76: FutureWarning: The default value of numeric_only in DataFrame.median is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df_out.fillna(df_out.median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "old_feat_col=feature_cols.copy()\n",
    "df_1=prepare_missing(df_1)\n",
    "df_1=prepare_outliers(df_1)\n",
    "\n",
    "\n",
    "# df_1=df_1.loc[df_1['city']=='SEATTLE']\n",
    "\n",
    "\n",
    "df_out,feature_cols_2=preprocessor(df_1, feature_cols)\n",
    "df_out[\"sale_year\"] = df_out[\"sale_date\"].dt.year\n",
    "df_out[\"sale_month\"] = df_out[\"sale_date\"].dt.month\n",
    "df_out[\"sale_week\"] = df_out[\"sale_date\"].dt.isocalendar().week\n",
    "df_out[\"sale_day\"] = df_out[\"sale_date\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7636302a-631d-48ab-9e8c-9da77f583601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132/3924654007.py:13: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  X_train=df_train[feature_cols].copy()\n",
      "/tmp/ipykernel_132/3924654007.py:16: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  X_test=df_test[feature_cols].copy()\n"
     ]
    }
   ],
   "source": [
    "df_train = df_out[(df_out[\"sale_date\"] < max_train_date) & (df_out[\"sale_date\"] >= min_train_date)].copy()\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_out[df_out[\"sale_date\"] >= max_train_date].copy()\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "response_transformer = ResponseTransformer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train=df_train[feature_cols].copy()\n",
    "y_train=response_transformer.fit_transform(df_train[response_col])\n",
    "\n",
    "X_test=df_test[feature_cols].copy()\n",
    "y_test=df_test[response_col].copy()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_train[list(X_train.columns)]=scaler.fit_transform(X_train)\n",
    "X_test[list(X_test.columns)]=scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0691deb-e0ba-4bf6-806c-1c39c066ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_percentage_error(true, pred, epsilon=1e-6):\n",
    "    \"\"\"Mean Percentage Error (MPE).\"\"\"\n",
    "    pe = (pred - true) / (true + epsilon)\n",
    "    return np.mean(pe)\n",
    "\n",
    "\n",
    "def median_absolute_percentage_error(true, pred, epsilon=1e-6):\n",
    "    \"\"\"Median Absolute Percentage Error (MdAPE).\"\"\"\n",
    "    ape = np.abs(pred - true) / (true + epsilon)\n",
    "    return np.median(ape)\n",
    "\n",
    "\n",
    "def median_percentage_error(true, pred, epsilon=1e-6):\n",
    "    \"\"\"Median Percentage Error (MdPE).\"\"\"\n",
    "    pe = (pred - true) / (true + epsilon)\n",
    "    return np.median(pe)\n",
    "\n",
    "\n",
    "def frac_within_ci(true, pred_low, pred_high):\n",
    "    \"\"\"Fraction within Confidence Interval.\"\"\"\n",
    "    wci = (true >= pred_low) & (true <= pred_high)\n",
    "    return np.mean(wci)\n",
    "\n",
    "\n",
    "def mean_ci_width(true, pred_low, pred_high, epsilon=1e-6):\n",
    "    \"\"\"Mean Confidence Interval Width.\"\"\"\n",
    "    ciw = (pred_high - pred_low) / (true + epsilon)\n",
    "    return np.mean(ciw)\n",
    "\n",
    "\n",
    "def median_ci_width(true, pred_low, pred_high, epsilon=1e-6):\n",
    "    \"\"\"Median Confidence Interval Width.\"\"\"\n",
    "    ciw = (pred_high - pred_low) / (true + epsilon)\n",
    "    return np.median(ciw)\n",
    "\n",
    "\n",
    "def evaluate_preds(y_test, y_pred, y_pred_low=None, y_pred_high=None):\n",
    "    output = {}\n",
    "    output[\"Count\"] = len(y_test)\n",
    "    output[\"R2\"] = r2_score(y_test, y_pred)\n",
    "    output[\"MAE\"] = mean_absolute_error(y_test, y_pred)\n",
    "    output[\"MdAE\"] = median_absolute_error(y_test, y_pred)\n",
    "    output[\"MPE\"] = mean_percentage_error(y_test, y_pred)\n",
    "    output[\"MdPE\"] = median_percentage_error(y_test, y_pred)\n",
    "    output[\"MAPE\"] = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    output[\"MdAPE\"] = median_absolute_percentage_error(y_test, y_pred)\n",
    "    if y_pred_low is not None and y_pred_high is not None:\n",
    "        output[\"Pct. Within CI\"] = frac_within_ci(y_test, y_pred_low, y_pred_high)\n",
    "        output[\"Mean CI Width\"] = mean_ci_width(y_test, y_pred_low, y_pred_high)\n",
    "        output[\"Med. CI Width\"] = median_ci_width(y_test, y_pred_low, y_pred_high)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7479217d-095b-49ee-ad46-4c19efaf0b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmostly referencing Reid's public notebook on kaggle for typical preprocessing steps \\nhttps://www.kaggle.com/code/reidjohnson/house-price-eda-and-modeling-with-python\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "mostly referencing Reid's public notebook on kaggle for typical preprocessing steps \n",
    "https://www.kaggle.com/code/reidjohnson/house-price-eda-and-modeling-with-python\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73a718c6-2895-4a27-a4ef-7b9365a7b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train[['latitude', 'longitude']]\n",
    "X_test=X_test[['latitude', 'longitude']]\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train.values, y_train.values)\n",
    "y_pred_knn=knn.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d811475-d6a5-42eb-bf41-58c310c75e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MdAE</th>\n",
       "      <th>MPE</th>\n",
       "      <th>MdPE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MdAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn_comps</th>\n",
       "      <td>30995</td>\n",
       "      <td>0.410641</td>\n",
       "      <td>338154.07411</td>\n",
       "      <td>214865.082271</td>\n",
       "      <td>-0.224494</td>\n",
       "      <td>-0.269288</td>\n",
       "      <td>0.288875</td>\n",
       "      <td>0.282975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Count        R2           MAE           MdAE       MPE      MdPE  \\\n",
       "knn_comps  30995  0.410641  338154.07411  214865.082271 -0.224494 -0.269288   \n",
       "\n",
       "               MAPE     MdAPE  \n",
       "knn_comps  0.288875  0.282975  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_knn= response_transformer.inverse_transform(y_pred_knn)\n",
    "evaluate_preds(y_test.values, y_pred_knn)\n",
    "pd.DataFrame([evaluate_preds(y_test, y_pred_knn)], index=[f\"{version}\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
