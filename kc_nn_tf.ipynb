{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0468d270-f5f7-47a9-bff5-8863cd82dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "version='kc_nn_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae23592-808d-4319-93da-00e4f2d859a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.6\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import collections\n",
    "import copy\n",
    "import csv\n",
    "import math\n",
    "import numbers\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "import types\n",
    "import warnings\n",
    "from functools import partial, update_wrapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader, NeighborSampler\n",
    "from torch_geometric.nn import SAGEConv, TransformerConv,NNConv\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from tqdm import tqdm\n",
    "import torch_geometric\n",
    "torch_geometric.__version__\n",
    "torch.__version__\n",
    "print(torch.version.cuda)\n",
    "import sys\n",
    "# sys.path\n",
    "# sys.path.append('zestimate-neural-net')\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.sql import DataFrame, SparkSession\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql import types as T\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# from aip_spark_sdk import AwsHelper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import csv\n",
    "import math\n",
    "# import numbers\n",
    "# import random\n",
    "# import threading\n",
    "# import time\n",
    "# import types\n",
    "import warnings\n",
    "from functools import partial, update_wrapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "# from sklearn.metrics import mean_absolute_percentage_error\n",
    "# from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import BallTree\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d56caba-1914-41f1-bc1c-cb1036d3694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict():\n",
    "    num_features = [\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"year_built\",\n",
    "        \"year_reno\",\n",
    "        \"grade\",\n",
    "        \"fbsmt_grade\",\n",
    "        \"condition\",\n",
    "        \"stories\",\n",
    "        \"beds\",\n",
    "        \"bath_full\",\n",
    "        \"bath_3qtr\",\n",
    "        \"bath_half\",\n",
    "    ]\n",
    "    num_log_features = [\n",
    "        \"land_val\",\n",
    "        \"imp_val\",\n",
    "        \"sqft_lot\",\n",
    "        \"sqft\",\n",
    "        \"sqft_1\",\n",
    "        \"sqft_fbsmt\",\n",
    "        \"garb_sqft\",\n",
    "        \"gara_sqft\",\n",
    "    ]\n",
    "    cat_features = [\n",
    "        # \"area\",\n",
    "        \"city\",\n",
    "       \"submarket\",\n",
    "        \"zoning\",\n",
    "        # \"present_use\",\n",
    "\n",
    "        # \"wfnt\",\n",
    "        # \"golf\",\n",
    "        # \"greenbelt\",\n",
    "        # \"noise_traffic\",\n",
    "        # \"view_rainier\",\n",
    "        # \"view_olympics\",\n",
    "        # \"view_cascades\",\n",
    "        # \"view_territorial\",\n",
    "        # \"view_skyline\",\n",
    "        # \"view_sound\",\n",
    "        # \"view_lakewash\",\n",
    "        # \"view_lakesamm\",\n",
    "        # \"view_otherwater\",\n",
    "        # \"view_other\",\n",
    "        \n",
    "        # \"sale_date_yyyymm\",\n",
    "        # \"sale_decade\",\n",
    "        # \"sale_year\",\n",
    "        # \"sale_week\",\n",
    "    ]\n",
    "    ord_features = [\n",
    "        # \"stories\",\n",
    "        # \"beds\",\n",
    "        # \"bath_full\",\n",
    "        # \"bath_3qtr\",\n",
    "        # \"bath_half\",\n",
    "    ]\n",
    "    time_features = [\"sale_date\",]\n",
    "\n",
    "    feature_dict = {\n",
    "        \"nums\": num_features,\n",
    "        \"num_logs\": num_log_features,\n",
    "        \"cats\": cat_features,\n",
    "        \"ords\": ord_features,\n",
    "        \"time\": time_features,\n",
    "    }\n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "\n",
    "feature_dict = get_feature_dict()\n",
    "\n",
    "id_cols = [\"sale_id\", \"pinx\", \"submarket\"]\n",
    "response_col = \"sale_price\"\n",
    "feature_cols = set(\n",
    "    feature_dict[\"nums\"] +\n",
    "    feature_dict[\"num_logs\"] +\n",
    "    feature_dict[\"cats\"] +\n",
    "    feature_dict[\"ords\"] +\n",
    "    feature_dict[\"time\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "031a7781-482d-4e45-9092-8a4359aadb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('kingcountysales.csv')\n",
    "df[\"sale_date\"]=pd.to_datetime(df[\"sale_date\"])\n",
    "max_train_date = \"2021-01-01\"\n",
    "min_train_date = \"2016-01-01\"\n",
    "sample_beg_date= \"2015-01-01\"\n",
    "df_1= df[df[\"sale_date\"] >= min_train_date].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "422f2047-152f-4ad3-a178-815c2d02656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_missing(df):\n",
    "    df = (\n",
    "        df\n",
    "        .assign(imp_val=lambda x: x['imp_val'].replace(0, np.nan))\n",
    "        .assign(land_val=lambda x: x['land_val'].replace(0, np.nan))\n",
    "        .assign(sqft=lambda x: x['sqft'].replace(0, np.nan))\n",
    "        .assign(beds=lambda x: x['beds'].replace(54, np.nan))\n",
    "    )\n",
    "    return df\n",
    "def clean_sqft(df):\n",
    "    sqft_1 = df.query('sqft < sqft_1')['sqft_1']\n",
    "    sqft_fbsmt = df.query('sqft < sqft_1')['sqft_fbsmt']\n",
    "    df.loc[df['sqft'] < df['sqft_1'], 'sqft'] = sqft_1 + sqft_fbsmt\n",
    "    return df\n",
    "\n",
    "def clean_year_built(df):\n",
    "    df['build_type'] = 'Standard'\n",
    "    df.loc[(df['year_built'] - 1) == pd.to_datetime(df['sale_date']).dt.year, 'build_type'] = 'New Construction'\n",
    "    df.loc[(df['year_built'] - 1) > pd.to_datetime(df['sale_date']).dt.year, 'build_type'] = 'Lot Sale'\n",
    "    return df\n",
    "\n",
    "def prepare_outliers(df):\n",
    "    df = (\n",
    "        df\n",
    "        .pipe(clean_sqft)\n",
    "        .pipe(clean_year_built)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def preprocessor(df_1, feature_c):\n",
    "    df_out=df_1.copy()\n",
    "    # feature_cols=feature_c.copy()\n",
    "    \n",
    "    '''\n",
    "    categorical features: replace each category with its price statistics from prior years  \n",
    "    '''\n",
    "    df_sample=  df[(df[\"sale_date\"] < min_train_date) & (df[\"sale_date\"] >= sample_beg_date)].copy()\n",
    "    trans_1=ResponseTransformer()\n",
    "    df_sample[response_col]=trans_1.fit_transform(df_sample[response_col].values)\n",
    "    # df_sample[response_col]= np.log1p(df_sample[response_col].values) - df_sample[response_col].median()\n",
    "    for cat_col in feature_dict['cats']:\n",
    "        cat_stats= df_sample[[cat_col,response_col]].groupby(cat_col).describe().add_prefix(f'{cat_col}_')[f'{cat_col}_sale_price'].reset_index()\n",
    "        df_out=df_out.merge(cat_stats, how='left', on=cat_col)\n",
    "        print(cat_col)\n",
    "        print(df_out.shape)\n",
    "        print(df_out['sale_date'].isna().sum())\n",
    "        df_out.drop(columns=cat_col, inplace=True)\n",
    "        feature_cols.update(list(cat_stats.columns))\n",
    "        feature_cols.remove(cat_col)\n",
    "        \n",
    "    '''\n",
    "    temporal adjusting \n",
    "    '''\n",
    "    print(df_out['sale_date'].isna().sum())\n",
    "    date_time = df_out['sale_date']\n",
    "\n",
    "    timestamp_s = date_time.dt.week\n",
    "    # df_1['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "    # df_1['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "    df_out['year_sin'] = np.sin(timestamp_s * (2 * np.pi / 52))\n",
    "    df_out['year_cos'] = np.cos(timestamp_s * (2 * np.pi / 52))\n",
    "    # df_out.drop(columns='sale_date', inplace=True)\n",
    "    df_out['trans_year']=date_time.dt.year\n",
    "    feature_cols.update(['trans_year','year_sin','year_cos'])\n",
    "    feature_cols.remove('sale_date')\n",
    "    \n",
    "    '''\n",
    "    taking log\n",
    "    '''\n",
    "    \n",
    "    df_out[feature_dict['num_logs']]=np.log1p(df_out[feature_dict['num_logs']])\n",
    "    '''\n",
    "    filling missing values\n",
    "    '''\n",
    "    \n",
    "    df_out.fillna(df_out.median(), inplace=True)\n",
    "    # print(df_out.isna().sum())\n",
    "    # df_out.dropna(inplace=True)\n",
    "    return df_out, feature_cols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a931304-3d76-45dd-ab3f-40e244633ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city\n",
      "(48180, 56)\n",
      "0\n",
      "submarket\n",
      "(48180, 63)\n",
      "0\n",
      "zoning\n",
      "(48180, 70)\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4480/1148910247.py:54: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  timestamp_s = date_time.dt.week\n",
      "/tmp/ipykernel_4480/1148910247.py:73: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  df_out.fillna(df_out.median(), inplace=True)\n",
      "/tmp/ipykernel_4480/1148910247.py:73: FutureWarning: The default value of numeric_only in DataFrame.median is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df_out.fillna(df_out.median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "old_feat_col=feature_cols.copy()\n",
    "df_1=prepare_missing(df_1)\n",
    "df_1=prepare_outliers(df_1)\n",
    "# df_1=df_1.loc[df_1['city']=='SEATTLE'] # change this line accordingly based on the experiment \n",
    "df_out,feature_cols_2=preprocessor(df_1, feature_cols)\n",
    "df_out[\"sale_year\"] = df_out[\"sale_date\"].dt.year\n",
    "df_out[\"sale_month\"] = df_out[\"sale_date\"].dt.month\n",
    "df_out[\"sale_week\"] = df_out[\"sale_date\"].dt.isocalendar().week\n",
    "df_out[\"sale_day\"] = df_out[\"sale_date\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8a9ee97-8cfa-4670-be17-f97e8ff532ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseTransformer:\n",
    "    \"\"\"Response transformer.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.median = None\n",
    "\n",
    "    def fit(self, y):\n",
    "        y_trans = np.log1p(y)\n",
    "        self.median = np.median(y_trans)\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        y = np.log1p(y) - self.median\n",
    "        return y\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        return self.fit(y).transform(y)\n",
    "\n",
    "    def inverse_transform(self, y):\n",
    "        y = np.expm1(y + self.median)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7636302a-631d-48ab-9e8c-9da77f583601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4480/676246714.py:13: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  X_train=df_train[feature_cols]\n",
      "/tmp/ipykernel_4480/676246714.py:16: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  X_test=df_test[feature_cols]\n",
      "/tmp/ipykernel_4480/676246714.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[list(X_train.columns)]=scaler.fit_transform(X_train)\n",
      "/tmp/ipykernel_4480/676246714.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[list(X_test.columns)]=scaler.transform(X_test)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_out[(df_out[\"sale_date\"] < max_train_date) & (df_out[\"sale_date\"] >= min_train_date)].copy()\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_out[df_out[\"sale_date\"] >= max_train_date].copy()\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "response_transformer = ResponseTransformer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train=df_train[feature_cols]\n",
    "y_train=response_transformer.fit_transform(df_train[response_col])\n",
    "\n",
    "X_test=df_test[feature_cols]\n",
    "y_test=df_test[response_col]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_train[list(X_train.columns)]=scaler.fit_transform(X_train)\n",
    "X_test[list(X_test.columns)]=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0691deb-e0ba-4bf6-806c-1c39c066ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_percentage_error(true, pred, epsilon=1e-6):\n",
    "    \"\"\"Mean Percentage Error (MPE).\"\"\"\n",
    "    pe = (pred - true) / (true + epsilon)\n",
    "    return np.mean(pe)\n",
    "\n",
    "\n",
    "def median_absolute_percentage_error(true, pred, epsilon=1e-6):\n",
    "    \"\"\"Median Absolute Percentage Error (MdAPE).\"\"\"\n",
    "    ape = np.abs(pred - true) / (true + epsilon)\n",
    "    return np.median(ape)\n",
    "\n",
    "\n",
    "def median_percentage_error(true, pred, epsilon=1e-6):\n",
    "    \"\"\"Median Percentage Error (MdPE).\"\"\"\n",
    "    pe = (pred - true) / (true + epsilon)\n",
    "    return np.median(pe)\n",
    "\n",
    "\n",
    "def frac_within_ci(true, pred_low, pred_high):\n",
    "    \"\"\"Fraction within Confidence Interval.\"\"\"\n",
    "    wci = (true >= pred_low) & (true <= pred_high)\n",
    "    return np.mean(wci)\n",
    "\n",
    "\n",
    "def mean_ci_width(true, pred_low, pred_high, epsilon=1e-6):\n",
    "    \"\"\"Mean Confidence Interval Width.\"\"\"\n",
    "    ciw = (pred_high - pred_low) / (true + epsilon)\n",
    "    return np.mean(ciw)\n",
    "\n",
    "\n",
    "def median_ci_width(true, pred_low, pred_high, epsilon=1e-6):\n",
    "    \"\"\"Median Confidence Interval Width.\"\"\"\n",
    "    ciw = (pred_high - pred_low) / (true + epsilon)\n",
    "    return np.median(ciw)\n",
    "\n",
    "\n",
    "def evaluate_preds(y_test, y_pred, y_pred_low=None, y_pred_high=None):\n",
    "    output = {}\n",
    "    output[\"Count\"] = len(y_test)\n",
    "    output[\"R2\"] = r2_score(y_test, y_pred)\n",
    "    output[\"MAE\"] = mean_absolute_error(y_test, y_pred)\n",
    "    output[\"MdAE\"] = median_absolute_error(y_test, y_pred)\n",
    "    output[\"MPE\"] = mean_percentage_error(y_test, y_pred)\n",
    "    output[\"MdPE\"] = median_percentage_error(y_test, y_pred)\n",
    "    output[\"MAPE\"] = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    output[\"MdAPE\"] = median_absolute_percentage_error(y_test, y_pred)\n",
    "    if y_pred_low is not None and y_pred_high is not None:\n",
    "        output[\"Pct. Within CI\"] = frac_within_ci(y_test, y_pred_low, y_pred_high)\n",
    "        output[\"Mean CI Width\"] = mean_ci_width(y_test, y_pred_low, y_pred_high)\n",
    "        output[\"Med. CI Width\"] = median_ci_width(y_test, y_pred_low, y_pred_high)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7479217d-095b-49ee-ad46-4c19efaf0b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmostly referencing Reid's public notebook on kaggle for typical preprocessing steps \\nhttps://www.kaggle.com/code/reidjohnson/house-price-eda-and-modeling-with-python\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "mostly referencing Reid's public notebook on kaggle for typical preprocessing steps \n",
    "https://www.kaggle.com/code/reidjohnson/house-price-eda-and-modeling-with-python\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "697a1f0a-6387-4144-8678-2377c720ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 15:54:48.988816: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-28 15:54:49.022077: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 15:54:51.304963: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X_train_tf=tf.convert_to_tensor(X_train.values)\n",
    "y_train_tf=tf.convert_to_tensor(y_train.values)\n",
    "X_test_tf=tf.convert_to_tensor(X_test.values)\n",
    "y_test_tf=tf.convert_to_tensor(y_test.values)\n",
    "y_test_tf=response_transformer.transform(y_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5936cdd5-6230-4247-8a3f-e1fccddc1320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.89608605, -0.27193329, -0.01333338, ...,  0.3038262 ,\n",
       "        0.25659118,  1.22417455])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d87526ff-bcaa-418f-94ea-0142c4831140",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data=tf.data.Dataset.from_tensor_slices(X_train_tf)\n",
    "label_data=tf.data.Dataset.from_tensor_slices(y_train_tf)\n",
    "\n",
    "train_data= tf.data.Dataset.zip((inp_data,label_data))\n",
    "train_data= train_data.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7a61663-9db1-4214-9c0a-c4b167fc9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def my_metric(y_true,_y_pred):\n",
    "    return median_absolute_percentage_error(y_true, response_transformer.inverse_transform(y_pred))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd633fe-681f-4781-8633-6406b93a537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dim=X_train_tf.shape[-1]\n",
    "\n",
    "model=tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=inp_dim))\n",
    "model.add(tf.keras.layers.Dense(512, activation='softplus'))\n",
    "model.add(tf.keras.layers.Dense(128, activation='softplus'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='softplus'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "opt=tf.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8e2756e-f6c9-4d3a-a904-7341f228b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               24576     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94401 (368.75 KB)\n",
      "Trainable params: 94401 (368.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6782d5b9-5dcf-4fbe-aa15-8f82bce61ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc3b3ef1-116a-4ee8-8b38-bab5306616c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "298/298 [==============================] - 2s 7ms/step - loss: 0.1975 - val_loss: 0.0989\n",
      "Epoch 2/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0494 - val_loss: 0.1121\n",
      "Epoch 3/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0450 - val_loss: 0.1125\n",
      "Epoch 4/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0446 - val_loss: 0.1095\n",
      "Epoch 5/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0419 - val_loss: 0.1067\n",
      "Epoch 6/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0413 - val_loss: 0.1004\n",
      "Epoch 7/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0385 - val_loss: 0.0987\n",
      "Epoch 8/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0378 - val_loss: 0.0954\n",
      "Epoch 9/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0373 - val_loss: 0.0916\n",
      "Epoch 10/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0372 - val_loss: 0.0912\n",
      "Epoch 11/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0365 - val_loss: 0.0890\n",
      "Epoch 12/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0360 - val_loss: 0.0870\n",
      "Epoch 13/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0355 - val_loss: 0.0849\n",
      "Epoch 14/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0346 - val_loss: 0.0838\n",
      "Epoch 15/40\n",
      "298/298 [==============================] - 2s 7ms/step - loss: 0.0345 - val_loss: 0.0827\n",
      "Epoch 16/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0342 - val_loss: 0.0810\n",
      "Epoch 17/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0340 - val_loss: 0.0795\n",
      "Epoch 18/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0337 - val_loss: 0.0787\n",
      "Epoch 19/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0333 - val_loss: 0.0793\n",
      "Epoch 20/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0330 - val_loss: 0.0822\n",
      "Epoch 21/40\n",
      "298/298 [==============================] - 2s 7ms/step - loss: 0.0327 - val_loss: 0.0848\n",
      "Epoch 22/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0325 - val_loss: 0.0862\n",
      "Epoch 23/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0324 - val_loss: 0.0820\n",
      "Epoch 24/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0322 - val_loss: 0.0855\n",
      "Epoch 25/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0321 - val_loss: 0.0876\n",
      "Epoch 26/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0320 - val_loss: 0.0817\n",
      "Epoch 27/40\n",
      "298/298 [==============================] - 2s 7ms/step - loss: 0.0318 - val_loss: 0.0832\n",
      "Epoch 28/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0315 - val_loss: 0.0831\n",
      "Epoch 29/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0313 - val_loss: 0.0843\n",
      "Epoch 30/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0312 - val_loss: 0.0842\n",
      "Epoch 31/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0311 - val_loss: 0.0799\n",
      "Epoch 32/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0310 - val_loss: 0.0871\n",
      "Epoch 33/40\n",
      "298/298 [==============================] - 2s 7ms/step - loss: 0.0310 - val_loss: 0.0849\n",
      "Epoch 34/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0309 - val_loss: 0.0837\n",
      "Epoch 35/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0309 - val_loss: 0.0832\n",
      "Epoch 36/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0309 - val_loss: 0.0838\n",
      "Epoch 37/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0309 - val_loss: 0.0746\n",
      "Epoch 38/40\n",
      "298/298 [==============================] - 2s 7ms/step - loss: 0.0309 - val_loss: 0.0734\n",
      "Epoch 39/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0308 - val_loss: 0.0747\n",
      "Epoch 40/40\n",
      "298/298 [==============================] - 2s 6ms/step - loss: 0.0306 - val_loss: 0.0802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f39406a5970>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,validation_data=[X_test_tf,y_test_tf], epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7399e4b1-fed5-44b5-bd3e-0c2e6da085db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 0s 984us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b8703a6-22bb-405e-b425-58d4dd5b588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_descaled= response_transformer.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf90ce6e-d00c-4e6b-a45a-4a74518a194e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MdAE</th>\n",
       "      <th>MPE</th>\n",
       "      <th>MdPE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MdAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kc_nn_v2</th>\n",
       "      <td>10096</td>\n",
       "      <td>0.56728</td>\n",
       "      <td>224301.003439</td>\n",
       "      <td>125106.28125</td>\n",
       "      <td>-0.095378</td>\n",
       "      <td>-0.112596</td>\n",
       "      <td>0.186864</td>\n",
       "      <td>0.148963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Count       R2            MAE          MdAE       MPE      MdPE  \\\n",
       "kc_nn_v2  10096  0.56728  224301.003439  125106.28125 -0.095378 -0.112596   \n",
       "\n",
       "              MAPE     MdAPE  \n",
       "kc_nn_v2  0.186864  0.148963  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=pd.DataFrame([evaluate_preds(y_test.values, y_pred_descaled.flatten())], index=[f\"{version}\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c95f29f-ab00-427d-b8c6-bb3f530c1753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30% 0.8273573692551506\n",
      "10% 0.34003565768621236\n"
     ]
    }
   ],
   "source": [
    "df_new=df_test.copy()\n",
    "# df_new['recordingdate']=pd.to_datetime(df_new['recordingdate'])\n",
    "# df_new['week_period']=df_new['recordingdate'].dt.to_period('w')\n",
    "df_new['y_pred']=y_pred_descaled.flatten()\n",
    "df_new['indive_mape']=np.abs(df_new['y_pred'].values-df_new[response_col].values)/df_new[response_col].values\n",
    "print('30%', len(df_new.loc[df_new['indive_mape']<=0.3])/len(df_new))\n",
    "print('10%', len(df_new.loc[df_new['indive_mape']<=0.1])/len(df_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60cf0c6-e140-4bcf-b483-09fe95ae7313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
